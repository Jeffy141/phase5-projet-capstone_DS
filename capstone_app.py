import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.model_selection import train_test_split # Assurez-vous que cette ligne est bien l√†!
import warnings
warnings.filterwarnings('ignore')

# Configuration de la page
st.set_page_config(
    page_title="Syst√®me de Maintenance Pr√©dictive IoT",
    page_icon="üè≠",
    layout="wide"
)

# Style pour les graphiques (on enl√®ve 'plt.style.use')
sns.set_palette("husl") # Gardez ceci pour les couleurs de seaborn

# --- CLASSE SYST√àME D'ALERTE INTELLIGENT ---
# ... le reste du code ...

# --- CLASSE SYST√àME D'ALERTE INTELLIGENT ---
class IntelligentAlertSystem:
    def __init__(self, model, features):
        self.model = model
        self.features = features

    def predict_with_confidence(self, sensor_data):
        """Pr√©diction avec niveau de confiance"""
        input_dict = {}
        for feature in self.features:
            if feature in sensor_data:
                input_dict[feature] = sensor_data[feature]
            else:
                input_dict[feature] = 0.0
        
        input_data = pd.DataFrame([input_dict])[self.features]
        prediction = self.model.predict(input_data)[0]
        probabilities = self.model.predict_proba(input_data)[0]
        confidence = np.max(probabilities)
        class_names = self.model.classes_
        proba_dict = dict(zip(class_names, probabilities))
        return prediction, confidence, proba_dict

    def evaluate_risk(self, sensor_id, prediction, confidence, sensor_data):
        """√âvaluation du niveau de risque"""
        risk_levels = {
            'HIGH': {'threshold': 0.85, 'action': 'MAINTENANCE IMM√âDIATE'},
            'MEDIUM': {'threshold': 0.70, 'action': 'SURVEILLANCE RENFORC√âE'},
            'LOW': {'threshold': 0.50, 'action': 'MONITORING STANDARD'}
        }
        
        if prediction != 'None' and confidence > risk_levels['HIGH']['threshold']:
            risk_level = 'HIGH'
        elif prediction != 'None' and confidence > risk_levels['MEDIUM']['threshold']:
            risk_level = 'MEDIUM'
        elif prediction != 'None':
            risk_level = 'LOW'
        else:
            risk_level = 'SAFE'
        
        recommendations = self._generate_recommendations(prediction, sensor_data)
        
        return {
            'sensor_id': sensor_id,
            'prediction': prediction,
            'confidence': confidence,
            'risk_level': risk_level,
            'action': risk_levels.get(risk_level, {}).get('action', 'AUCUNE'),
            'recommendations': recommendations
        }

    def _generate_recommendations(self, prediction, sensor_data):
        """G√©n√©ration de recommandations personnalis√©es"""
        recommendations = {
            'None': ["‚úÖ Syst√®me op√©rationnel", "üìÖ Maintenance planifi√©e normale"],
            'Electrical Fault': [
                "üîå V√©rifier l'alimentation √©lectrique",
                "üîé Inspecter les connexions",
                "‚ö° Contr√¥ler la stabilit√© de tension"
            ],
            'Mechanical Failure': [
                "üõë Arr√™t imm√©diat recommand√©",
                "üîß Inspection m√©canique compl√®te",
                "üìä V√©rifier l'usure des composants"
            ],
            'Overheating': [
                "üå°Ô∏è R√©duire la charge imm√©diatement",
                "‚ùÑÔ∏è V√©rifier le syst√®me de refroidissement",
                "üî• Temp√©rature critique d√©tect√©e"
            ]
        }
        return recommendations.get(prediction, ["üîç Diagnostic √† approfondir"])

# --- CHARGEMENT DES DONN√âES ---
@st.cache_data
def load_csv_data():
    """Charge le fichier CSV r√©el"""
    try:
        df = pd.read_csv('iot_equipment_monitoring_dataset.csv')
        st.success(f"‚úÖ Fichier CSV charg√© : {df.shape[0]} lignes, {df.shape[1]} colonnes")
        return df
    except FileNotFoundError:
        st.warning("üìÅ Fichier CSV non trouv√©. G√©n√©ration de donn√©es simul√©es...")
        return generate_sample_data()

def generate_sample_data():
    """G√©n√®re des donn√©es simul√©es"""
    np.random.seed(42)
    n_samples = 5000
    
    data = {
        'Sensor_ID': [f'SENSOR_{i:03d}' for i in range(n_samples)],
        'Temperature': np.random.normal(65, 15, n_samples),
        'Vibration': np.random.gamma(2, 2, n_samples),
        'Pressure': np.random.normal(100, 20, n_samples),
        'Voltage': np.random.normal(220, 10, n_samples),
        'Current': np.random.normal(15, 3, n_samples),
        'FFT_Feature1': np.random.uniform(0, 1, n_samples),
        'FFT_Feature2': np.random.uniform(0, 1, n_samples),
        'Anomaly_Score': np.random.uniform(0, 1, n_samples),
        'Timestamp': pd.date_range('2024-01-01', periods=n_samples, freq='H')
    }
    
    df = pd.DataFrame(data)
    
    conditions = [
        (df['Temperature'] > 85) & (df['Voltage'] < 210),
        (df['Vibration'] > 8) & (df['Pressure'] > 130),
        (df['Temperature'] > 90),
        (df['Current'] > 20)
    ]
    choices = ['Electrical Fault', 'Mechanical Failure', 'Overheating', 'Electrical Fault']
    
    df['Fault_Type'] = np.select(conditions, choices, default='None')
    df['Fault_Status'] = (df['Fault_Type'] != 'None').astype(int)
    
    return df

# --- CHARGEMENT ET PR√âPARATION DES DONN√âES ---
@st.cache_resource
def load_model_and_data():
    """Charge les donn√©es et entra√Æne le mod√®le"""
    st.info("üéØ Chargement des donn√©es et entra√Ænement du mod√®le...")
    
    df_raw = load_csv_data()
    df_clean = df_raw.copy()
    
    # Nettoyage
    if 'Fault_Type' in df_clean.columns:
        df_clean['Fault_Type'] = df_clean['Fault_Type'].fillna('None')
    else:
        df_clean['Fault_Type'] = 'None'
    
    # Feature engineering
    if 'Timestamp' in df_clean.columns:
        df_clean['Timestamp'] = pd.to_datetime(df_clean['Timestamp'])
        df_clean['hour'] = df_clean['Timestamp'].dt.hour
    else:
        df_clean['hour'] = 12
    
    df_clean['thermal_stress'] = df_clean['Temperature'] * df_clean['Current'] / 1000
    df_clean['mechanical_stress'] = df_clean['Vibration'] * df_clean['Pressure'] / 1000
    df_clean['power_consumption'] = df_clean['Voltage'] * df_clean['Current']
    
    # Normalisation
    for col in ['Temperature', 'Vibration', 'Pressure', 'Voltage', 'Current']:
        if col in df_clean.columns:
            normalized_col = f'Normalized_{col}'
            df_clean[normalized_col] = (df_clean[col] - df_clean[col].mean()) / df_clean[col].std()
    
    # Features finales
    base_features = ['Temperature', 'Vibration', 'Pressure', 'Voltage', 'Current']
    optional_features = ['FFT_Feature1', 'FFT_Feature2', 'Anomaly_Score']
    computed_features = [
        'Normalized_Temperature', 'Normalized_Vibration', 'Normalized_Pressure',
        'Normalized_Voltage', 'Normalized_Current', 'hour',
        'thermal_stress', 'mechanical_stress', 'power_consumption'
    ]
    
    features = []
    for feature_list in [base_features, optional_features, computed_features]:
        for feature in feature_list:
            if feature in df_clean.columns:
                features.append(feature)
    
    # Compl√©ter les features manquantes
    for feature in features:
        if feature not in df_clean.columns:
            df_clean[feature] = 0.0
    
    X = df_clean[features]
    y = df_clean['Fault_Type']

# Mod√®le ensemble
    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
    xgb_model = XGBClassifier(random_state=42)
    
    ensemble_model = VotingClassifier(
        estimators=[('rf', rf_model), ('xgb', xgb_model)],
        voting='soft'
    )

    # üü¢ √âTAPE CRUCIALE : Entra√Æner le mod√®le !
    ensemble_model.fit(X, y) 
    
    # Initialiser le syst√®me d'alerte
    alert_system = IntelligentAlertSystem(ensemble_model, features)
    
    st.success("ü§ñ Mod√®le entra√Æn√© et pr√™t √† l'emploi !")
    
    # üü¢ RETOURNER TOUTES LES VARIABLES DANS L'ORDRE ATTENDU
    return alert_system, df_clean, ensemble_model, features, X, y
    
# --- FONCTIONS DE VISUALISATION ---
def create_confusion_matrix(model, X, y):
    """Cr√©e la matrice de confusion"""
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    y_pred = model.predict(X_test)
    
    fig, ax = plt.subplots(figsize=(10, 8))
    cm = confusion_matrix(y_test, y_pred, labels=model.classes_)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_, ax=ax)
    ax.set_title('MATRICE DE CONFUSION', fontsize=16, fontweight='bold')
    ax.set_xlabel('Pr√©dictions')
    ax.set_ylabel('Valeurs R√©elles')
    
    # Ajouter l'accuracy
    accuracy = accuracy_score(y_test, y_pred)
    ax.text(0.5, -0.1, f'Accuracy: {accuracy:.3f}', transform=ax.transAxes, 
            ha='center', fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
    
    return fig

def create_feature_importance(model, features):
    """Cr√©e le graphique d'importance des features"""
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
    else:
        # Pour VotingClassifier, prendre la moyenne des importances
        importances = np.mean([est.feature_importances_ for est in model.estimators_ if hasattr(est, 'feature_importances_')], axis=0)
    
    feature_imp = pd.DataFrame({
        'feature': features,
        'importance': importances
    }).sort_values('importance', ascending=False)
    
    fig, ax = plt.subplots(figsize=(12, 8))
    sns.barplot(data=feature_imp.head(15), y='feature', x='importance', ax=ax)
    ax.set_title('TOP 15 - IMPORTANCE DES VARIABLES', fontsize=16, fontweight='bold')
    ax.set_xlabel('Importance')
    return fig, feature_imp

def create_fault_distribution(df):
    """Cr√©e la distribution des d√©fauts"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Camembert
    fault_dist = df['Fault_Type'].value_counts()
    colors = ['lightgreen', 'red', 'orange', 'yellow']
    ax1.pie(fault_dist.values, labels=fault_dist.index, autopct='%1.1f%%', 
            startangle=90, colors=colors[:len(fault_dist)])
    ax1.set_title('Distribution des Types de D√©fauts', fontweight='bold')
    
    # Barres
    sns.barplot(x=fault_dist.index, y=fault_dist.values, ax=ax2, palette=colors[:len(fault_dist)])
    ax2.set_title('Nombre de D√©fauts par Type', fontweight='bold')
    ax2.tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    return fig

def create_sensor_risk_analysis(df):
    """Analyse des capteurs √† risque"""
    sensor_analysis = df.groupby('Sensor_ID').agg({
        'Fault_Status': ['count', 'sum', 'mean'], 
        'Temperature': 'max', 
        'Vibration': 'max'
    }).round(3)
    
    sensor_analysis.columns = ['total_mesures', 'defauts_total', 'taux_defaut', 'temp_max', 'vibration_max']
    high_risk_sensors = sensor_analysis[sensor_analysis['taux_defaut'] > 0.1].sort_values('taux_defaut', ascending=False)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Top 10 capteurs √† risque
    top_10 = high_risk_sensors['taux_defaut'].head(10)
    colors = ['red' if x > 0.3 else 'orange' for x in top_10.values]
    top_10.plot(kind='bar', ax=ax1, color=colors)
    ax1.set_title('TOP 10 - TAUX DE D√âFAUTS PAR CAPTEUR', fontweight='bold')
    ax1.set_ylabel('Taux de D√©fauts')
    ax1.tick_params(axis='x', rotation=45)
    
    # Scatter plot temp√©rature vs vibration
    scatter = sns.scatterplot(data=sensor_analysis, x='temp_max', y='vibration_max',
                   size='taux_defaut', hue='taux_defaut', sizes=(20, 200), ax=ax2, palette='viridis')
    ax2.set_title('PROFIL DES CAPTEURS - TEMP√âRATURE vs VIBRATION', fontweight='bold')
    ax2.set_xlabel('Temp√©rature Max (¬∞C)')
    ax2.set_ylabel('Vibration Max (m/s¬≤)')
    
    plt.tight_layout()
    return fig, high_risk_sensors.head(10)

def create_correlation_heatmap(df):
    """Cr√©e la heatmap de corr√©lation"""
    numeric_features = ['Temperature', 'Vibration', 'Pressure', 'Voltage', 'Current', 
                       'FFT_Feature1', 'FFT_Feature2', 'Anomaly_Score', 'Fault_Status']
    numeric_features = [f for f in numeric_features if f in df.columns]
    
    fig, ax = plt.subplots(figsize=(12, 10))
    correlation_matrix = df[numeric_features].corr()
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                square=True, linewidths=0.5, fmt='.2f', ax=ax, cbar_kws={"shrink": .8})
    ax.set_title('MATRICE DE CORR√âLATION - VARIABLES IoT', fontsize=16, fontweight='bold')
    return fig

def create_sensor_visualization(sensor_data, prediction):
    """Cr√©e la visualisation des donn√©es du capteur"""
    fig = plt.figure(figsize=(15, 10))
    
    # Graphique radar pour les param√®tres principaux
    ax1 = fig.add_subplot(221, polar=True)
    parameters = ['Temperature', 'Vibration', 'Pressure', 'Voltage', 'Current']
    values = [sensor_data.get(p, 0) for p in parameters]
    
    # Normalisation pour le radar
    max_vals = [120, 15, 200, 250, 25]  # Valeurs maximales typiques
    normalized_vals = [v/max_vals[i] for i, v in enumerate(values)]
    
    angles = np.linspace(0, 2*np.pi, len(parameters), endpoint=False).tolist()
    angles += angles[:1]
    normalized_vals += normalized_vals[:1]
    
    ax1.plot(angles, normalized_vals, 'o-', linewidth=2, label='Capteur')
    ax1.fill(angles, normalized_vals, alpha=0.25)
    ax1.set_thetagrids(np.degrees(angles[:-1]), parameters)
    ax1.set_title('PROFIL DU CAPTEUR (Radar)', size=14, fontweight='bold')
    ax1.grid(True)
    
    # Barres des valeurs actuelles
    ax2 = fig.add_subplot(222)
    colors = ['red' if v > max_vals[i]*0.8 else 'green' for i, v in enumerate(values)]
    bars = ax2.bar(parameters, values, color=colors, alpha=0.7)
    ax2.set_title('VALEURS DES CAPTEURS', fontweight='bold')
    ax2.set_ylabel('Valeurs')
    ax2.tick_params(axis='x', rotation=45)
    
    # Ajouter les valeurs sur les barres
    for bar, value in zip(bars, values):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                f'{value:.1f}', ha='center', va='bottom')
    
    # Seuils de danger
    danger_thresholds = [85, 8, 130, 210, 20]
    for i, (param, danger) in enumerate(zip(parameters, danger_thresholds)):
        ax2.axhline(y=danger, color='red', linestyle='--', alpha=0.5)
        ax2.text(len(parameters)-0.5, danger + (i*2), f'Seuil {param}: {danger}', 
                fontsize=8, color='red', ha='right')
    
    # Distribution temporelle simul√©e
    ax3 = fig.add_subplot(223)
    time_points = np.arange(24)
    temp_trend = sensor_data['Temperature'] + np.random.normal(0, 5, 24)
    vib_trend = sensor_data['Vibration'] + np.random.normal(0, 1, 24)
    
    ax3.plot(time_points, temp_trend, label='Temp√©rature', color='red', linewidth=2)
    ax3.set_ylabel('Temp√©rature (¬∞C)', color='red')
    ax3.tick_params(axis='y', labelcolor='red')
    ax3.set_xlabel('Heures')
    
    ax3_twin = ax3.twinx()
    ax3_twin.plot(time_points, vib_trend, label='Vibration', color='blue', linewidth=2)
    ax3_twin.set_ylabel('Vibration (m/s¬≤)', color='blue')
    ax3_twin.tick_params(axis='y', labelcolor='blue')
    
    ax3.set_title('TENDANCE TEMPORELLE (24h)', fontweight='bold')
    ax3.legend(loc='upper left')
    ax3_twin.legend(loc='upper right')
    
    # Statut de pr√©diction
    ax4 = fig.add_subplot(224)
    status_colors = {'HIGH': 'red', 'MEDIUM': 'orange', 'LOW': 'yellow', 'SAFE': 'green'}
    status = 'HIGH' if prediction != 'None' else 'SAFE'
    
    ax4.text(0.5, 0.6, f'STATUT: {prediction}', ha='center', va='center', 
             fontsize=20, fontweight='bold', color=status_colors.get(status, 'black'))
    ax4.text(0.5, 0.3, f'NIVEAU: {status}', ha='center', va='center', 
             fontsize=16, color=status_colors.get(status, 'black'))
    ax4.set_xlim(0, 1)
    ax4.set_ylim(0, 1)
    ax4.axis('off')
    ax4.set_title('DIAGNOSTIC ACTUEL', fontweight='bold')
    
    plt.tight_layout()
    return fig

def create_performance_dashboard(model, X, y):
    """Cr√©e un dashboard de performance complet"""
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    y_pred = model.predict(X_test)
    
    # M√©triques de performance
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, output_dict=True)
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # Accuracy par classe
    classes = model.classes_
    class_accuracy = []
    for cls in classes:
        if cls in report:
            class_accuracy.append(report[cls]['precision'])
    
    ax1.bar(classes, class_accuracy, color=['lightblue', 'lightcoral', 'lightgreen', 'yellow'])
    ax1.set_title('PR√âCISION PAR CLASSE', fontweight='bold')
    ax1.set_ylabel('Pr√©cision')
    ax1.tick_params(axis='x', rotation=45)
    
    # Distribution des probabilit√©s
    y_proba = model.predict_proba(X_test)
    proba_df = pd.DataFrame(y_proba, columns=classes)
    proba_df['true_class'] = y_test.reset_index(drop=True)
    
    for cls in classes:
        cls_proba = proba_df[proba_df['true_class'] == cls][cls]
        ax2.hist(cls_proba, alpha=0.6, label=cls, bins=20)
    
    ax2.set_title('DISTRIBUTION DES PROBABILIT√âS', fontweight='bold')
    ax2.set_xlabel('Probabilit√©')
    ax2.set_ylabel('Fr√©quence')
    ax2.legend()
    
    # Learning curve (simul√©e)
    train_sizes = np.linspace(0.1, 1.0, 10)
    train_scores = []
    test_scores = []
    
    for size in train_sizes:
        n_train = int(size * len(X_train))
        model.fit(X_train[:n_train], y_train[:n_train])
        train_scores.append(accuracy_score(y_train[:n_train], model.predict(X_train[:n_train])))
        test_scores.append(accuracy_score(y_test, model.predict(X_test)))
    
    ax3.plot(train_sizes, train_scores, 'o-', label='Train', color='blue')
    ax3.plot(train_sizes, test_scores, 'o-', label='Test', color='red')
    ax3.set_title('COURBE D\'APPRENTISSAGE', fontweight='bold')
    ax3.set_xlabel('Taille de l\'ensemble d\'entra√Ænement')
    ax3.set_ylabel('Accuracy')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # M√©triques globales
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    values = [
        accuracy,
        report['weighted avg']['precision'],
        report['weighted avg']['recall'],
        report['weighted avg']['f1-score']
    ]
    
    colors = ['lightblue', 'lightgreen', 'lightcoral', 'gold']
    bars = ax4.bar(metrics, values, color=colors)
    ax4.set_title('M√âTRIQUES GLOBALES DU MOD√àLE', fontweight='bold')
    ax4.set_ylabel('Score')
    ax4.set_ylim(0, 1)
    
    # Ajouter les valeurs sur les barres
    for bar, value in zip(bars, values):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                f'{value:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    return fig

# --- INTERFACE PRINCIPALE ---
def main():
    st.title("üè≠ Syst√®me de Maintenance Pr√©dictive IoT")
    st.markdown("**Projet Capstone - Pr√©sentation Compl√®te avec Visualisations**")
    
    # Barre de progression
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # Chargement des donn√©es
    try:
        status_text.text("üéØ Chargement des donn√©es...")
        progress_bar.progress(25)
        
        alert_system, df_clean, model, features, X, y = load_model_and_data()
        
        status_text.text("‚úÖ Donn√©es charg√©es avec succ√®s!")
        progress_bar.progress(100)
        
    except Exception as e:
        st.error(f"Erreur lors du chargement : {e}")
        return
    
    # Supprimer la barre de progression apr√®s chargement
    progress_bar.empty()
    status_text.empty()
    
    # Onglets pour l'organisation
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "üéØ Simulation", 
        "üìä Analyse Donn√©es", 
        "ü§ñ Performance Mod√®le",
        "üìà Graphiques Pr√©sentation",
        "üèÜ Top 10 Capteurs",
        "üìã Rapport Complet"
    ])
    
    with tab1:
        st.header("üîç Simulation en Temps R√©el")
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader("üìä Param√®tres du Capteur")
            
            # Contr√¥les avanc√©s
            temperature = st.slider("üå°Ô∏è Temperature (¬∞C)", 20.0, 120.0, 65.0, 0.1)
            vibration = st.slider("üìä Vibration (m/s¬≤)", 0.0, 15.0, 3.5, 0.1)
            pressure = st.slider("üí® Pressure (kPa)", 50.0, 200.0, 95.0, 0.1)
            voltage = st.slider("‚ö° Voltage (V)", 180.0, 250.0, 220.0, 0.1)
            current = st.slider("üîã Current (A)", 5.0, 25.0, 15.0, 0.1)
            
            # Exemples pr√©d√©finis pour d√©mo
            scenario = st.selectbox("Sc√©nario de d√©monstration :", 
                                  ["Normal", "D√©faut √âlectrique", "D√©faut M√©canique", "Surchauffe"])
            
            if scenario == "D√©faut √âlectrique":
                temperature, vibration, pressure, voltage, current = 75.0, 4.0, 110.0, 190.0, 22.0
            elif scenario == "D√©faut M√©canique":
                temperature, vibration, pressure, voltage, current = 70.0, 9.5, 140.0, 215.0, 16.0
            elif scenario == "Surchauffe":
                temperature, vibration, pressure, voltage, current = 92.0, 5.0, 105.0, 225.0, 18.0
        
        with col2:
            st.subheader("üéØ R√©sultats du Diagnostic")
            
            if st.button("üöÄ Lancer le Diagnostic", type="primary", use_container_width=True):
                # Pr√©paration des donn√©es
                sensor_data = {}
                for feature in features:
                    if feature == 'Temperature': sensor_data[feature] = temperature
                    elif feature == 'Vibration': sensor_data[feature] = vibration
                    elif feature == 'Pressure': sensor_data[feature] = pressure
                    elif feature == 'Voltage': sensor_data[feature] = voltage
                    elif feature == 'Current': sensor_data[feature] = current
                    elif feature == 'thermal_stress': sensor_data[feature] = temperature * current / 1000
                    elif feature == 'mechanical_stress': sensor_data[feature] = vibration * pressure / 1000
                    elif feature == 'power_consumption': sensor_data[feature] = voltage * current
                    elif feature == 'hour': sensor_data[feature] = pd.Timestamp.now().hour
                    else: sensor_data[feature] = 0.0
                
                # Pr√©diction
                with st.spinner("üîç Analyse en cours..."):
                    prediction, confidence, proba_dict = alert_system.predict_with_confidence(sensor_data)
                    alert_info = alert_system.evaluate_risk("SENSOR_001", prediction, confidence, sensor_data)
                
                # R√©sultats
                col1, col2, col3 = st.columns(3)
                
                risk_color = {
                    'HIGH': 'red', 
                    'MEDIUM': 'orange', 
                    'LOW': 'yellow', 
                    'SAFE': 'green'
                }
                
                col1.metric("Pr√©diction", alert_info['prediction'])
                col2.metric("Niveau de Risque", alert_info['risk_level'])
                col3.metric("Confiance", f"{alert_info['confidence']:.1%}")
                
                # Alerte
                if alert_info['risk_level'] == 'HIGH':
                    st.error(f"üö® **{alert_info['action']}**")
                elif alert_info['risk_level'] == 'MEDIUM':
                    st.warning(f"‚ö†Ô∏è **{alert_info['action']}**")
                elif alert_info['risk_level'] == 'LOW':
                    st.info(f"‚ÑπÔ∏è **{alert_info['action']}**")
                else:
                    st.success(f"‚úÖ **{alert_info['action']}**")
                
                # Graphique de visualisation du capteur
                st.subheader("üìà Visualisation des Donn√©es du Capteur")
                fig_sensor = create_sensor_visualization(sensor_data, prediction)
                st.pyplot(fig_sensor)
                
                # Recommandations et probabilit√©s
                col_rec, col_prob = st.columns(2)
                
                with col_rec:
                    with st.expander("üìã Recommandations d√©taill√©es", expanded=True):
                        for i, rec in enumerate(alert_info['recommendations'], 1):
                            st.write(f"{i}. {rec}")
                
                with col_prob:
                    with st.expander("üìä Probabilit√©s par type de d√©faut"):
                        prob_df = pd.DataFrame(proba_dict, index=['Probabilit√©']).T
                        prob_df_sorted = prob_df.sort_values('Probabilit√©', ascending=False)
                        st.dataframe(prob_df_sorted.style.format("{:.1%}").background_gradient(cmap='RdYlGn_r'))
    
    with tab2:
        st.header("üìä Analyse Exploratoire des Donn√©es")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Distribution des D√©fauts")
            fig_dist = create_fault_distribution(df_clean)
            st.pyplot(fig_dist)
        
        with col2:
            st.subheader("Matrice de Corr√©lation")
            fig_corr = create_correlation_heatmap(df_clean)
            st.pyplot(fig_corr)
        
        # Statistiques descriptives
        st.subheader("üìã Statistiques Descriptives")
        numeric_cols = ['Temperature', 'Vibration', 'Pressure', 'Voltage', 'Current']
        numeric_cols = [col for col in numeric_cols if col in df_clean.columns]
        st.dataframe(df_clean[numeric_cols].describe().style.background_gradient(cmap='Blues'))
    
    with tab3:
        st.header("ü§ñ Performance du Mod√®le")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Matrice de Confusion")
            fig_cm = create_confusion_matrix(model, X, y)
            st.pyplot(fig_cm)
        
        with col2:
            st.subheader("Importance des Features")
            fig_imp, feature_imp = create_feature_importance(model, features)
            st.pyplot(fig_imp)
            
            st.subheader("Top 10 Features")
            st.dataframe(feature_imp.head(10).style.background_gradient(cmap='Greens'))
        
        # Dashboard de performance
        st.subheader("üìä Dashboard de Performance Complet")
        fig_perf = create_performance_dashboard(model, X, y)
        st.pyplot(fig_perf)
    
    with tab4:
        st.header("üìà Graphiques pour Pr√©sentation")
        
        st.info("üéØ **Graphiques pr√™ts pour votre pr√©sentation**")
        
        # Graphique 1: Distribution des d√©fauts
        st.subheader("1. Distribution des Types de D√©fauts")
        fig1 = create_fault_distribution(df_clean)
        st.pyplot(fig1)
        
        # Graphique 2: Matrice de confusion
        st.subheader("2. Matrice de Confusion du Mod√®le")
        fig2 = create_confusion_matrix(model, X, y)
        st.pyplot(fig2)
        
        # Graphique 3: Importance des features
        st.subheader("3. Importance des Variables")
        fig3, _ = create_feature_importance(model, features)
        st.pyplot(fig3)
        
        # Graphique 4: Corr√©lations
        st.subheader("4. Analyse des Corr√©lations")
        fig4 = create_correlation_heatmap(df_clean)
        st.pyplot(fig4)
    
    with tab5:
        st.header("üèÜ Top 10 Capteurs √† Risque")
        
        fig_risk, top_sensors = create_sensor_risk_analysis(df_clean)
        st.pyplot(fig_risk)
        
        st.subheader("üìã Liste D√©taill√©e des Capteurs √† Risque")
        st.dataframe(top_sensors.style.background_gradient(cmap='Reds', subset=['taux_defaut']))
        
        # Explication des r√©sultats
        st.subheader("üìù Analyse des R√©sultats")
        st.markdown("""
        **Interpr√©tation :**
        - üî¥ **Capteurs √† haut risque** : Taux de d√©faut > 10%
        - üü° **Risque moyen** : Taux de d√©faut 5-10%  
        - üü¢ **Faible risque** : Taux de d√©faut < 5%
        
        **Recommandations :**
        - Prioriser la maintenance sur les capteurs en rouge
        - Surveiller les tendances des capteurs en orange
        - Maintenir la surveillance standard pour les verts
        """)
    
    with tab6:
        st.header("üìã Rapport Complet du Syst√®me")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("üìà M√©triques Cl√©s")
            
            # Calcul des m√©triques
            total_sensors = df_clean['Sensor_ID'].nunique()
            fault_rate = df_clean['Fault_Status'].mean() * 100
            avg_temperature = df_clean['Temperature'].mean()
            avg_vibration = df_clean['Vibration'].mean()
            
            st.metric("Nombre total de capteurs", total_sensors)
            st.metric("Taux de d√©faut global", f"{fault_rate:.1f}%")
            st.metric("Temp√©rature moyenne", f"{avg_temperature:.1f}¬∞C")
            st.metric("Vibration moyenne", f"{avg_vibration:.1f} m/s¬≤")
        
        with col2:
            st.subheader("üéØ Performance du Mod√®le")
            
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            
            st.metric("Accuracy globale", f"{accuracy:.3f}")
            st.metric("Nombre de features", len(features))
            st.metric("Classes pr√©dites", len(model.classes_))
        
        # R√©sum√© ex√©cutif
        st.subheader("üìã R√©sum√© Ex√©cutif")
        st.markdown(f"""
        **Syst√®me de Maintenance Pr√©dictive IoT - Rapport de Performance**
        
        - **üìä Couverture des donn√©es** : {len(df_clean)} √©chantillons analys√©s
        - **ü§ñ Performance du mod√®le** : {accuracy:.1%} de pr√©cision
        - **üîß Types de d√©fauts d√©tect√©s** : {len(model.classes_)}
        - **üéØ Capteurs √† risque identifi√©s** : {len(top_sensors) if 'top_sensors' in locals() else 'N/A'}
        - **üí° Recommandations** : Syst√®me op√©rationnel et pr√™t pour le d√©ploiement
        
        **Points forts :**
        ‚úÖ D√©tection pr√©coce des d√©fauts  
        ‚úÖ Interface intuitive  
        ‚úÖ Mod√®le ensemble robuste  
        ‚úÖ Visualisations compl√®tes  
        """)

# Sidebar avec informations
with st.sidebar:
    st.header("üìä √Ä propos")
    st.markdown("""
    **Technologies utilis√©es :**
    - Python, Scikit-learn, XGBoost
    - Random Forest, SVM, Mod√®le Ensemble
    - Streamlit pour l'interface
    
    **Graphiques inclus :**
    - üìà Matrice de confusion
    - üìä Importance des features
    - üéØ Distribution des d√©fauts
    - üîó Corr√©lations
    - üèÜ Top 10 capteurs √† risque
    - üìâ Visualisation temps r√©el
    """)
    
    st.header("üéØ Pour la Pr√©sentation")
    st.markdown("""
    **Points cl√©s √† montrer :**
    1. Simulation temps r√©el
    2. Performance du mod√®le
    3. Analyse des donn√©es
    4. Graphiques explicatifs
    5. Recommandations business
    """)
    
    # Ajouter un s√©lecteur de th√®me
    st.header("üé® Personnalisation")
    theme = st.selectbox("Choisir le th√®me des graphiques:", 
                        ["Default", "Dark", "Pastel", "Bright"])
    
    if theme == "Dark":
        plt.style.use('dark_background')
        sns.set_palette("husl")
    elif theme == "Pastel":
        sns.set_palette("pastel")
    elif theme == "Bright":
        sns.set_palette("bright")

if __name__ == "__main__":
    main()
