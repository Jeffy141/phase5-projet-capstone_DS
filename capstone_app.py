import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, classification_report
import warnings
warnings.filterwarnings('ignore')

# Configuration de la page
st.set_page_config(
    page_title="SystÃ¨me de Maintenance PrÃ©dictive IoT",
    page_icon="ğŸ­",
    layout="wide"
)

# Style pour les graphiques
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# --- CLASSE SYSTÃˆME D'ALERTE INTELLIGENT ---
class IntelligentAlertSystem:
    def __init__(self, model, features):
        self.model = model
        self.features = features

    def predict_with_confidence(self, sensor_data):
        """PrÃ©diction avec niveau de confiance"""
        input_dict = {}
        for feature in self.features:
            if feature in sensor_data:
                input_dict[feature] = sensor_data[feature]
            else:
                input_dict[feature] = 0.0
        
        input_data = pd.DataFrame([input_dict])[self.features]
        prediction = self.model.predict(input_data)[0]
        probabilities = self.model.predict_proba(input_data)[0]
        confidence = np.max(probabilities)
        class_names = self.model.classes_
        proba_dict = dict(zip(class_names, probabilities))
        return prediction, confidence, proba_dict

    def evaluate_risk(self, sensor_id, prediction, confidence, sensor_data):
        """Ã‰valuation du niveau de risque"""
        risk_levels = {
            'HIGH': {'threshold': 0.85, 'action': 'MAINTENANCE IMMÃ‰DIATE'},
            'MEDIUM': {'threshold': 0.70, 'action': 'SURVEILLANCE RENFORCÃ‰E'},
            'LOW': {'threshold': 0.50, 'action': 'MONITORING STANDARD'}
        }
        
        if prediction != 'None' and confidence > risk_levels['HIGH']['threshold']:
            risk_level = 'HIGH'
        elif prediction != 'None' and confidence > risk_levels['MEDIUM']['threshold']:
            risk_level = 'MEDIUM'
        elif prediction != 'None':
            risk_level = 'LOW'
        else:
            risk_level = 'SAFE'
        
        recommendations = self._generate_recommendations(prediction, sensor_data)
        
        return {
            'sensor_id': sensor_id,
            'prediction': prediction,
            'confidence': confidence,
            'risk_level': risk_level,
            'action': risk_levels.get(risk_level, {}).get('action', 'AUCUNE'),
            'recommendations': recommendations
        }

    def _generate_recommendations(self, prediction, sensor_data):
        """GÃ©nÃ©ration de recommandations personnalisÃ©es"""
        recommendations = {
            'None': ["âœ… SystÃ¨me opÃ©rationnel", "ğŸ“… Maintenance planifiÃ©e normale"],
            'Electrical Fault': [
                "ğŸ”Œ VÃ©rifier l'alimentation Ã©lectrique",
                "ğŸ” Inspecter les connexions",
                "âš¡ ContrÃ´ler la stabilitÃ© de tension"
            ],
            'Mechanical Failure': [
                "ğŸ›‘ ArrÃªt immÃ©diat recommandÃ©",
                "ğŸ”§ Inspection mÃ©canique complÃ¨te",
                "ğŸ“Š VÃ©rifier l'usure des composants"
            ],
            'Overheating': [
                "ğŸŒ¡ï¸ RÃ©duire la charge immÃ©diatement",
                "â„ï¸ VÃ©rifier le systÃ¨me de refroidissement",
                "ğŸ”¥ TempÃ©rature critique dÃ©tectÃ©e"
            ]
        }
        return recommendations.get(prediction, ["ğŸ” Diagnostic Ã  approfondir"])

# --- CHARGEMENT DES DONNÃ‰ES ---
@st.cache_data
def load_csv_data():
    """Charge le fichier CSV rÃ©el"""
    try:
        df = pd.read_csv('iot_equipment_monitoring_dataset.csv')
        st.success(f"âœ… Fichier CSV chargÃ© : {df.shape[0]} lignes, {df.shape[1]} colonnes")
        return df
    except FileNotFoundError:
        st.warning("ğŸ“ Fichier CSV non trouvÃ©. GÃ©nÃ©ration de donnÃ©es simulÃ©es...")
        return generate_sample_data()

def generate_sample_data():
    """GÃ©nÃ¨re des donnÃ©es simulÃ©es"""
    np.random.seed(42)
    n_samples = 5000
    
    data = {
        'Sensor_ID': [f'SENSOR_{i:03d}' for i in range(n_samples)],
        'Temperature': np.random.normal(65, 15, n_samples),
        'Vibration': np.random.gamma(2, 2, n_samples),
        'Pressure': np.random.normal(100, 20, n_samples),
        'Voltage': np.random.normal(220, 10, n_samples),
        'Current': np.random.normal(15, 3, n_samples),
        'FFT_Feature1': np.random.uniform(0, 1, n_samples),
        'FFT_Feature2': np.random.uniform(0, 1, n_samples),
        'Anomaly_Score': np.random.uniform(0, 1, n_samples),
        'Timestamp': pd.date_range('2024-01-01', periods=n_samples, freq='H')
    }
    
    df = pd.DataFrame(data)
    
    conditions = [
        (df['Temperature'] > 85) & (df['Voltage'] < 210),
        (df['Vibration'] > 8) & (df['Pressure'] > 130),
        (df['Temperature'] > 90),
        (df['Current'] > 20)
    ]
    choices = ['Electrical Fault', 'Mechanical Failure', 'Overheating', 'Electrical Fault']
    
    df['Fault_Type'] = np.select(conditions, choices, default='None')
    df['Fault_Status'] = (df['Fault_Type'] != 'None').astype(int)
    
    return df

# --- CHARGEMENT ET PRÃ‰PARATION DES DONNÃ‰ES ---
@st.cache_resource
def load_model_and_data():
    """Charge les donnÃ©es et entraÃ®ne le modÃ¨le"""
    st.info("ğŸ¯ Chargement des donnÃ©es et entraÃ®nement du modÃ¨le...")
    
    df_raw = load_csv_data()
    df_clean = df_raw.copy()
    
    # Nettoyage
    if 'Fault_Type' in df_clean.columns:
        df_clean['Fault_Type'] = df_clean['Fault_Type'].fillna('None')
    else:
        df_clean['Fault_Type'] = 'None'
    
    # Feature engineering
    if 'Timestamp' in df_clean.columns:
        df_clean['Timestamp'] = pd.to_datetime(df_clean['Timestamp'])
        df_clean['hour'] = df_clean['Timestamp'].dt.hour
    else:
        df_clean['hour'] = 12
    
    df_clean['thermal_stress'] = df_clean['Temperature'] * df_clean['Current'] / 1000
    df_clean['mechanical_stress'] = df_clean['Vibration'] * df_clean['Pressure'] / 1000
    df_clean['power_consumption'] = df_clean['Voltage'] * df_clean['Current']
    
    # Normalisation
    for col in ['Temperature', 'Vibration', 'Pressure', 'Voltage', 'Current']:
        if col in df_clean.columns:
            normalized_col = f'Normalized_{col}'
            df_clean[normalized_col] = (df_clean[col] - df_clean[col].mean()) / df_clean[col].std()
    
    # Features finales
    base_features = ['Temperature', 'Vibration', 'Pressure', 'Voltage', 'Current']
    optional_features = ['FFT_Feature1', 'FFT_Feature2', 'Anomaly_Score']
    computed_features = [
        'Normalized_Temperature', 'Normalized_Vibration', 'Normalized_Pressure',
        'Normalized_Voltage', 'Normalized_Current', 'hour',
        'thermal_stress', 'mechanical_stress', 'power_consumption'
    ]
    
    features = []
    for feature_list in [base_features, optional_features, computed_features]:
        for feature in feature_list:
            if feature in df_clean.columns:
                features.append(feature)
    
    # ComplÃ©ter les features manquantes
    for feature in features:
        if feature not in df_clean.columns:
            df_clean[feature] = 0.0
    
    X = df_clean[features]
    y = df_clean['Fault_Type']

    # ModÃ¨le ensemble
    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
    xgb_model = XGBClassifier(random_state=42)
    svm_model = SVC(probability=True, random_state=42)
    
    ensemble_model = VotingClassifier(
        estimators=[('rf', rf_model), ('xgb', xgb_model), ('svm', svm_model)],
        voting='soft'
    )

    ensemble_model.fit(X, y)
    alert_system = IntelligentAlertSystem(ensemble_model, features)
    
    st.success("âœ… ModÃ¨le entraÃ®nÃ© avec succÃ¨s !")
    return alert_system, df_clean, ensemble_model, features, X, y

# --- FONCTIONS DE VISUALISATION ---
def create_confusion_matrix(model, X, y):
    """CrÃ©e la matrice de confusion"""
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    y_pred = model.predict(X_test)
    
    fig, ax = plt.subplots(figsize=(10, 8))
    cm = confusion_matrix(y_test, y_pred, labels=model.classes_)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_, ax=ax)
    ax.set_title('MATRICE DE CONFUSION', fontsize=16, fontweight='bold')
    ax.set_xlabel('PrÃ©dictions')
    ax.set_ylabel('Valeurs RÃ©elles')
    return fig

def create_feature_importance(model, features):
    """CrÃ©e le graphique d'importance des features"""
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
    else:
        # Pour VotingClassifier, prendre la moyenne des importances
        importances = np.mean([est.feature_importances_ for est in model.estimators_ if hasattr(est, 'feature_importances_')], axis=0)
    
    feature_imp = pd.DataFrame({
        'feature': features,
        'importance': importances
    }).sort_values('importance', ascending=False)
    
    fig, ax = plt.subplots(figsize=(12, 8))
    sns.barplot(data=feature_imp.head(15), y='feature', x='importance', ax=ax)
    ax.set_title('TOP 15 - IMPORTANCE DES VARIABLES', fontsize=16, fontweight='bold')
    ax.set_xlabel('Importance')
    return fig, feature_imp

def create_fault_distribution(df):
    """CrÃ©e la distribution des dÃ©fauts"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Camembert
    fault_dist = df['Fault_Type'].value_counts()
    ax1.pie(fault_dist.values, labels=fault_dist.index, autopct='%1.1f%%', startangle=90)
    ax1.set_title('Distribution des Types de DÃ©fauts')
    
    # Barres
    sns.barplot(x=fault_dist.index, y=fault_dist.values, ax=ax2)
    ax2.set_title('Nombre de DÃ©fauts par Type')
    ax2.tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    return fig

def create_sensor_risk_analysis(df):
    """Analyse des capteurs Ã  risque"""
    sensor_analysis = df.groupby('Sensor_ID').agg({
        'Fault_Status': ['count', 'sum', 'mean'], 
        'Temperature': 'max', 
        'Vibration': 'max'
    }).round(3)
    
    sensor_analysis.columns = ['total_mesures', 'defauts_total', 'taux_defaut', 'temp_max', 'vibration_max']
    high_risk_sensors = sensor_analysis[sensor_analysis['taux_defaut'] > 0.1].sort_values('taux_defaut', ascending=False)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Top 10 capteurs Ã  risque
    high_risk_sensors['taux_defaut'].head(10).plot(kind='bar', ax=ax1, color='red')
    ax1.set_title('TOP 10 - TAUX DE DÃ‰FAUTS PAR CAPTEUR')
    ax1.set_ylabel('Taux de DÃ©fauts')
    
    # Scatter plot tempÃ©rature vs vibration
    sns.scatterplot(data=sensor_analysis, x='temp_max', y='vibration_max',
                   size='taux_defaut', hue='taux_defaut', sizes=(20, 200), ax=ax2)
    ax2.set_title('PROFIL DES CAPTEURS - TEMPÃ‰RATURE vs VIBRATION')
    
    plt.tight_layout()
    return fig, high_risk_sensors.head(10)

def create_correlation_heatmap(df):
    """CrÃ©e la heatmap de corrÃ©lation"""
    numeric_features = ['Temperature', 'Vibration', 'Pressure', 'Voltage', 'Current', 
                       'FFT_Feature1', 'FFT_Feature2', 'Anomaly_Score']
    numeric_features = [f for f in numeric_features if f in df.columns]
    
    fig, ax = plt.subplots(figsize=(12, 10))
    correlation_matrix = df[numeric_features].corr()
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                square=True, linewidths=0.5, fmt='.2f', ax=ax)
    ax.set_title('MATRICE DE CORRÃ‰LATION - VARIABLES IoT', fontsize=16, fontweight='bold')
    return fig

def create_sensor_visualization(sensor_data, prediction):
    """CrÃ©e la visualisation des donnÃ©es du capteur"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # Graphique radar pour les paramÃ¨tres principaux
    parameters = ['Temperature', 'Vibration', 'Pressure', 'Voltage', 'Current']
    values = [sensor_data.get(p, 0) for p in parameters]
    
    # Normalisation pour le radar
    max_vals = [120, 15, 200, 250, 25]  # Valeurs maximales typiques
    normalized_vals = [v/max_vals[i] for i, v in enumerate(values)]
    
    angles = np.linspace(0, 2*np.pi, len(parameters), endpoint=False).tolist()
    angles += angles[:1]
    normalized_vals += normalized_vals[:1]
    
    ax1 = plt.subplot(221, polar=True)
    ax1.plot(angles, normalized_vals, 'o-', linewidth=2, label='Capteur')
    ax1.fill(angles, normalized_vals, alpha=0.25)
    ax1.set_thetagrids(np.degrees(angles[:-1]), parameters)
    ax1.set_title('PROFIL DU CAPTEUR (Radar)', size=14, fontweight='bold')
    ax1.grid(True)
    
    # Barres des valeurs actuelles
    ax2 = plt.subplot(222)
    colors = ['red' if v > max_vals[i]*0.8 else 'green' for i, v in enumerate(values)]
    bars = ax2.bar(parameters, values, color=colors, alpha=0.7)
    ax2.set_title('VALEURS DES CAPTEURS', fontweight='bold')
    ax2.set_ylabel('Valeurs')
    ax2.tick_params(axis='x', rotation=45)
    
    # Ajouter les valeurs sur les barres
    for bar, value in zip(bars, values):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                f'{value:.1f}', ha='center', va='bottom')
    
    # Seuils de danger
    danger_thresholds = [85, 8, 130, 210, 20]
    for i, (param, danger) in enumerate(zip(parameters, danger_thresholds)):
        ax2.axhline(y=danger, color='red', linestyle='--', alpha=0.5)
        ax2.text(i, danger + 1, f'Seuil {param}', fontsize=8, color='red')
    
    # Distribution temporelle simulÃ©e
    ax3 = plt.subplot(223)
    time_points = np.arange(24)
    temp_trend = sensor_data['Temperature'] + np.random.normal(0, 5, 24)
    vib_trend = sensor_data['Vibration'] + np.random.normal(0, 1, 24)
    
    ax3.plot(time_points, temp_trend, label='TempÃ©rature', color='red', linewidth=2)
    ax3.set_ylabel('TempÃ©rature (Â°C)', color='red')
    ax3.tick_params(axis='y', labelcolor='red')
    ax3.set_xlabel('Heures')
    
    ax3_twin = ax3.twinx()
    ax3_twin.plot(time_points, vib_trend, label='Vibration', color='blue', linewidth=2)
    ax3_twin.set_ylabel('Vibration (m/sÂ²)', color='blue')
    ax3_twin.tick_params(axis='y', labelcolor='blue')
    
    ax3.set_title('TENDANCE TEMPORELLE (24h)', fontweight='bold')
    ax3.legend(loc='upper left')
    ax3_twin.legend(loc='upper right')
    
    # Statut de prÃ©diction
    ax4 = plt.subplot(224)
    status_colors = {'HIGH': 'red', 'MEDIUM': 'orange', 'LOW': 'yellow', 'SAFE': 'green'}
    status = 'HIGH' if prediction != 'None' else 'SAFE'
    
    ax4.text(0.5, 0.6, f'STATUT: {prediction}', ha='center', va='center', 
             fontsize=20, fontweight='bold', color=status_colors.get(status, 'black'))
    ax4.text(0.5, 0.3, f'NIVEAU: {status}', ha='center', va='center', 
             fontsize=16, color=status_colors.get(status, 'black'))
    ax4.set_xlim(0, 1)
    ax4.set_ylim(0, 1)
    ax4.axis('off')
    ax4.set_title('DIAGNOSTIC ACTUEL', fontweight='bold')
    
    plt.tight_layout()
    return fig

# --- INTERFACE PRINCIPALE ---
def main():
    st.title("ğŸ­ SystÃ¨me de Maintenance PrÃ©dictive IoT")
    st.markdown("**Projet Capstone - PrÃ©sentation ComplÃ¨te avec Visualisations**")
    
    # Chargement des donnÃ©es
    try:
        alert_system, df_clean, model, features, X, y = load_model_and_data()
    except Exception as e:
        st.error(f"Erreur lors du chargement : {e}")
        return
    
    # Onglets pour l'organisation
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ¯ Simulation", 
        "ğŸ“Š Analyse DonnÃ©es", 
        "ğŸ¤– Performance ModÃ¨le",
        "ğŸ“ˆ Graphiques PrÃ©sentation",
        "ğŸ† Top 10 Capteurs"
    ])
    
    with tab1:
        st.header("ğŸ” Simulation en Temps RÃ©el")
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader("ğŸ“Š ParamÃ¨tres du Capteur")
            
            # ContrÃ´les avancÃ©s
            temperature = st.slider("ğŸŒ¡ï¸ Temperature (Â°C)", 20.0, 120.0, 65.0, 0.1)
            vibration = st.slider("ğŸ“Š Vibration (m/sÂ²)", 0.0, 15.0, 3.5, 0.1)
            pressure = st.slider("ğŸ’¨ Pressure (kPa)", 50.0, 200.0, 95.0, 0.1)
            voltage = st.slider("âš¡ Voltage (V)", 180.0, 250.0, 220.0, 0.1)
            current = st.slider("ğŸ”‹ Current (A)", 5.0, 25.0, 15.0, 0.1)
            
            # Exemples prÃ©dÃ©finis pour dÃ©mo
            scenario = st.selectbox("ScÃ©nario de dÃ©monstration :", 
                                  ["Normal", "DÃ©faut Ã‰lectrique", "DÃ©faut MÃ©canique", "Surchauffe"])
            
            if scenario == "DÃ©faut Ã‰lectrique":
                temperature, vibration, pressure, voltage, current = 75.0, 4.0, 110.0, 190.0, 22.0
            elif scenario == "DÃ©faut MÃ©canique":
                temperature, vibration, pressure, voltage, current = 70.0, 9.5, 140.0, 215.0, 16.0
            elif scenario == "Surchauffe":
                temperature, vibration, pressure, voltage, current = 92.0, 5.0, 105.0, 225.0, 18.0
        
        with col2:
            st.subheader("ğŸ¯ RÃ©sultats du Diagnostic")
            
            if st.button("ğŸš€ Lancer le Diagnostic", type="primary", use_container_width=True):
                # PrÃ©paration des donnÃ©es
                sensor_data = {}
                for feature in features:
                    if feature == 'Temperature': sensor_data[feature] = temperature
                    elif feature == 'Vibration': sensor_data[feature] = vibration
                    elif feature == 'Pressure': sensor_data[feature] = pressure
                    elif feature == 'Voltage': sensor_data[feature] = voltage
                    elif feature == 'Current': sensor_data[feature] = current
                    elif feature == 'thermal_stress': sensor_data[feature] = temperature * current / 1000
                    elif feature == 'mechanical_stress': sensor_data[feature] = vibration * pressure / 1000
                    elif feature == 'power_consumption': sensor_data[feature] = voltage * current
                    elif feature == 'hour': sensor_data[feature] = pd.Timestamp.now().hour
                    else: sensor_data[feature] = 0.0
                
                # PrÃ©diction
                with st.spinner("ğŸ” Analyse en cours..."):
                    prediction, confidence, proba_dict = alert_system.predict_with_confidence(sensor_data)
                    alert_info = alert_system.evaluate_risk("SENSOR_001", prediction, confidence, sensor_data)
                
                # RÃ©sultats
                col1, col2, col3 = st.columns(3)
                col1.metric("PrÃ©diction", alert_info['prediction'])
                col2.metric("Niveau de Risque", alert_info['risk_level'])
                col3.metric("Confiance", f"{alert_info['confidence']:.1%}")
                
                # Alerte
                if alert_info['risk_level'] == 'HIGH':
                    st.error(f"ğŸš¨ **{alert_info['action']}**")
                elif alert_info['risk_level'] == 'MEDIUM':
                    st.warning(f"âš ï¸ **{alert_info['action']}**")
                elif alert_info['risk_level'] == 'LOW':
                    st.info(f"â„¹ï¸ **{alert_info['action']}**")
                else:
                    st.success(f"âœ… **{alert_info['action']}**")
                
                # Graphique de visualisation du capteur
                st.subheader("ğŸ“ˆ Visualisation des DonnÃ©es du Capteur")
                fig_sensor = create_sensor_visualization(sensor_data, prediction)
                st.pyplot(fig_sensor)
                
                # Recommandations et probabilitÃ©s
                col_rec, col_prob = st.columns(2)
                
                with col_rec:
                    with st.expander("ğŸ“‹ Recommandations dÃ©taillÃ©es", expanded=True):
                        for i, rec in enumerate(alert_info['recommendations'], 1):
                            st.write(f"{i}. {rec}")
                
                with col_prob:
                    with st.expander("ğŸ“Š ProbabilitÃ©s par type de dÃ©faut"):
                        prob_df = pd.DataFrame(proba_dict, index=['ProbabilitÃ©']).T
                        prob_df_sorted = prob_df.sort_values('ProbabilitÃ©', ascending=False)
                        st.dataframe(prob_df_sorted.style.format("{:.1%}"))
    
    with tab2:
        st.header("ğŸ“Š Analyse Exploratoire des DonnÃ©es")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Distribution des DÃ©fauts")
            fig_dist = create_fault_distribution(df_clean)
            st.pyplot(fig_dist)
        
        with col2:
            st.subheader("Matrice de CorrÃ©lation")
            fig_corr = create_correlation_heatmap(df_clean)
            st.pyplot(fig_corr)
        
        # Statistiques descriptives
        st.subheader("ğŸ“‹ Statistiques Descriptives")
        st.dataframe(df_clean[['Temperature', 'Vibration', 'Pressure', 'Voltage', 'Current']].describe())
    
    with tab3:
        st.header("ğŸ¤– Performance du ModÃ¨le")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Matrice de Confusion")
            fig_cm = create_confusion_matrix(model, X, y)
            st.pyplot(fig_cm)
        
        with col2:
            st.subheader("Importance des Features")
            fig_imp, feature_imp = create_feature_importance(model, features)
            st.pyplot(fig_imp)
            
            st.subheader("Top 10 Features")
            st.dataframe(feature_imp.head(10))
    
    with tab4:
        st.header("ğŸ“ˆ Graphiques pour PrÃ©sentation")
        
        st.info("ğŸ¯ **Graphiques prÃªts pour votre prÃ©sentation**")
        
        # Graphique 1: Distribution des dÃ©fauts
        st.subheader("1. Distribution des Types de DÃ©fauts")
        fig1 = create_fault_distribution(df_clean)
        st.pyplot(fig1)
        
        # Graphique 2: Matrice de confusion
        st.subheader("2. Matrice de Confusion du ModÃ¨le")
        fig2 = create_confusion_matrix(model, X, y)
        st.pyplot(fig2)
        
        # Graphique 3: Importance des features
        st.subheader("3. Importance des Variables")
        fig3, _ = create_feature_importance(model, features)
        st.pyplot(fig3)
        
        # Graphique 4: CorrÃ©lations
        st.subheader("4. Analyse des CorrÃ©lations")
        fig4 = create_correlation_heatmap(df_clean)
        st.pyplot(fig4)
    
    with tab5:
        st.header("ğŸ† Top 10 Capteurs Ã  Risque")
        
        fig_risk, top_sensors = create_sensor_risk_analysis(df_clean)
        st.pyplot(fig_risk)
        
        st.subheader("ğŸ“‹ Liste DÃ©taillÃ©e des Capteurs Ã  Risque")
        st.dataframe(top_sensors)
        
        # Explication des rÃ©sultats
        st.subheader("ğŸ“ Analyse des RÃ©sultats")
        st.markdown("""
        **InterprÃ©tation :**
        - ğŸ”´ **Capteurs Ã  haut risque** : Taux de dÃ©faut > 10%
        - ğŸŸ¡ **Risque moyen** : Taux de dÃ©faut 5-10%  
        - ğŸŸ¢ **Faible risque** : Taux de dÃ©faut < 5%
        
        **Recommandations :**
        - Prioriser la maintenance sur les capteurs en rouge
        - Surveiller les tendances des capteurs en orange
        - Maintenir la surveillance standard pour les verts
        """)

# Sidebar avec informations
with st.sidebar:
    st.header("ğŸ“Š Ã€ propos")
    st.markdown("""
    **Technologies utilisÃ©es :**
    - Python, Scikit-learn, XGBoost
    - Random Forest, SVM, ModÃ¨le Ensemble
    - Streamlit pour l'interface
    
    **Graphiques inclus :**
    - ğŸ“ˆ Matrice de confusion
    - ğŸ“Š Importance des features
    - ğŸ¯ Distribution des dÃ©fauts
    - ğŸ”— CorrÃ©lations
    - ğŸ† Top 10 capteurs Ã  risque
    - ğŸ“‰ Visualisation temps rÃ©el
    """)
    
    st.header("ğŸ¯ Pour la PrÃ©sentation")
    st.markdown("""
    **Points clÃ©s Ã  montrer :**
    1. Simulation temps rÃ©el
    2. Performance du modÃ¨le
    3. Analyse des donnÃ©es
    4. Graphiques explicatifs
    5. Recommandations business
    """)

if __name__ == "__main__":
    main()
